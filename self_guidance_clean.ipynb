{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d062fb-2ec8-452c-8d90-d0286d942a23",
   "metadata": {},
   "source": [
    "# Diffusion self-guidance for controllable image generation\n",
    "\n",
    "This notebook is an unofficial implementation of the [Diffusion Self-Guidance for Controllable Image Generation](https://arxiv.org/abs/2306.00986). If you are reading this and want to use it, my suggestion is to take this implementation as a start rather than an end â€” it works in some cases, but more research is needed to get guaranteed results for each kind of edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53dc0c-b3c5-4c4f-b20c-211af3c4dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn import init\n",
    "from diffusers import LMSDiscreteScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "# from miniai.core import *\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5aafee-e571-4976-b02b-c9ce63631a99",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1abc70-2d51-4cc4-b789-eef43218088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dims_right(x,y):\n",
    "    dim = y.ndim - x.ndim\n",
    "    return x[(...,) + (None,)*dim]\n",
    "\n",
    "def add_dims_left(x, y):\n",
    "    dim = y.ndim - x.ndim\n",
    "    return x[(None,)*dim + (...,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4277c9-a37c-4012-a25c-89ce0cd3d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(prompt, concat_unconditional=False, device='cpu'):\n",
    "    text_input = tokeniser(prompt, padding=\"max_length\", max_length=tokeniser.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    with torch.no_grad():\n",
    "        embeds = text_encoder(text_input.input_ids)[0]\n",
    "        if concat_unconditional:\n",
    "            uncond_input = tokeniser([\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "            uncond_embeddings = text_encoder(uncond_input.input_ids)[0]\n",
    "            embeds = torch.cat([uncond_embeddings, embeds])\n",
    "    return embeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ae626-ffc5-41d2-9397-2b02cc096d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img(input_img):\n",
    "    if len(input_img.shape)<4: input_img = input_img.unsqueeze(0)\n",
    "    with torch.no_grad(): latent = vae.encode(input_img*2 - 1)\n",
    "    return 0.18215 * latent.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b96fd-e9ff-40b0-ae72-8287cab91a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image): return (image.clip(-1,1) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f210145-0f72-49cd-be68-6b73fb120286",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.delegates(plt.Axes.imshow)\n",
    "def show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    if fc.hasattrs(im, ('cpu','permute','detach')):\n",
    "        im = im.detach().cpu()\n",
    "        if len(im.shape)==3 and im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=np.array(im)\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, **kwargs)\n",
    "    if title is not None: ax.set_title(title)\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    if noframe: ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2211d-a0ae-4158-9523-8769f95a1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.delegates(subplots)\n",
    "def get_grid(\n",
    "    n:int, # Number of axes\n",
    "    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n",
    "    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n",
    "    title:str=None, # If passed, title set to the figure\n",
    "    weight:str='bold', # Title font weight\n",
    "    size:int=14, # Title font size\n",
    "    **kwargs,\n",
    "): # fig and axs\n",
    "    \"Return a grid of `n` axes, `rows` by `cols`\"\n",
    "    if nrows: ncols = ncols or int(np.floor(n/nrows))\n",
    "    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n",
    "    else:\n",
    "        nrows = int(math.sqrt(n))\n",
    "        ncols = int(np.floor(n/nrows))\n",
    "    fig,axs = subplots(nrows, ncols, **kwargs)\n",
    "    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n",
    "    if title is not None: fig.suptitle(title, weight=weight, size=size)\n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9125689-7f7d-4ed4-a8c7-12707833dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.delegates(plt.subplots, keep=True)\n",
    "def subplots(\n",
    "    nrows:int=1, # Number of rows in returned axes grid\n",
    "    ncols:int=1, # Number of columns in returned axes grid\n",
    "    figsize:tuple=None, # Width, height in inches of the returned figure\n",
    "    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n",
    "    suptitle:str=None, # Title to be set to returned figure\n",
    "    **kwargs\n",
    "): # fig and axs\n",
    "    \"A figure and set of subplots to display images of `imsize` inches\"\n",
    "    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n",
    "    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    if suptitle is not None: fig.suptitle(suptitle)\n",
    "    if nrows*ncols==1: ax = np.array([ax])\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daf504-ef04-494b-903f-bd1197069768",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.delegates(subplots)\n",
    "def show_images(ims:list, # Images to show\n",
    "                nrows:int=None, # Number of rows in grid\n",
    "                ncols:int=None, # Number of columns in grid (auto-calculated if None)\n",
    "                titles:list=None, # Optional list of titles for each image\n",
    "                **kwargs):\n",
    "    \"Show all images `ims` as subplots with `rows` using `titles`\"\n",
    "    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n",
    "    for im,t,ax in zip_longest(ims, titles or [], axs): show_image(im, ax=ax, title=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b2c2e-f829-401e-9c3f-d806756245e9",
   "metadata": {},
   "source": [
    "#### Attention and activation collection/storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e74ceb-8ce6-4c4e-9b11-b40f1e00c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention_processor import AttnProcessor, Attention\n",
    "\n",
    "def get_features(hook, layer, inp, out):\n",
    "    if not hasattr(hook, 'feats'): hook.feats = out\n",
    "    hook.feats = out\n",
    "\n",
    "class Hook():\n",
    "    def __init__(self, model, func): self.hook = model.register_forward_hook(partial(func, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "def get_attn_dict(processor, model):\n",
    "    attn_procs = {}\n",
    "    for name in model.attn_processors.keys():\n",
    "        attn_procs[name] = processor(name=name)\n",
    "    return attn_procs\n",
    "\n",
    "class AttnStorage:\n",
    "    def __init__(self): self.storage = {}\n",
    "    def __call__(self, attention_map, name, pred_type='orig'): \n",
    "        if not name in self.storage: self.storage[name] = {}\n",
    "        self.storage[name][pred_type] = attention_map\n",
    "    def flush(self): self.storage = {}\n",
    "\n",
    "class CustomAttnProcessor(AttnProcessor):\n",
    "    def __init__(self, attn_storage, name=None): \n",
    "        fc.store_attr()\n",
    "        self.store = False\n",
    "        self.type = \"attn2\" if \"attn2\" in name else \"attn1\"\n",
    "    def set_storage(self, store, pred_type): \n",
    "        self.store = store\n",
    "        self.pred_type = pred_type\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "     \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        attention_probs.requires_grad_(True)\n",
    "        \n",
    "        if self.store: self.attn_storage(attention_probs, self.name, pred_type=self.pred_type) ## stores the attention maps in attn_storage\n",
    "        \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "def prepare_attention(model, attn_storage, pred_type='orig', set_store=True):\n",
    "    for name, module in model.attn_processors.items(): module.set_storage(set_store, pred_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e419a0-93dd-4e57-b2a8-8896aa130d8a",
   "metadata": {},
   "source": [
    "#### Self guidance equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1ec81-646e-42c4-bf08-c0374c00badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x): return (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80563788-f815-42e8-90a4-3926f6353d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_attention(attn, s=10):\n",
    "    norm_attn = s * (normalise(attn) - 0.5)\n",
    "    return normalise(norm_attn.sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30527cfc-5578-46fe-9b41-325755dd21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(attn, s=20): return threshold_attention(attn, s)\n",
    "def get_size(attn): return 1/attn.shape[-2] * threshold_attention(attn).sum((1,2)).mean()\n",
    "def get_centroid(attn):\n",
    "    if not len(attn.shape) == 3: attn = attn[:,:,None]\n",
    "    h = w = int(tensor(attn.shape[-2]).sqrt().item())\n",
    "    hs = torch.arange(h).view(-1, 1, 1).to(attn.device)\n",
    "    ws = torch.arange(w).view(1, -1, 1).to(attn.device)\n",
    "    attn = rearrange(attn.mean(0), '(h w) d -> h w d', h=h)\n",
    "    weighted_w = torch.sum(ws * attn, dim=[0,1])\n",
    "    weighted_h = torch.sum(hs * attn, dim=[0,1])\n",
    "    return torch.stack([weighted_w, weighted_h]) / attn.sum((0,1))\n",
    "def get_appearance(attn, feats):\n",
    "    if not len(attn.shape) == 3: attn = attn[:,:,None]\n",
    "    h = w = int(tensor(attn.shape[-2]).sqrt().item())\n",
    "    shape = get_shape(attn).detach().mean(0).view(h,w,attn.shape[-1])\n",
    "    feats = feats.mean((0,1))[:,:,None]\n",
    "    return (shape*feats).sum() / shape.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c760b2-6a04-422e-909d-ab86d7061b61",
   "metadata": {},
   "source": [
    "#### G functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5105c4d-f515-4bfa-8898-1a225ca04e3c",
   "metadata": {},
   "source": [
    "Single image editing. These are the functions that are closest to the paper. In the experiments section below, I played around with variations on these equations in pursuit of better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d6ef1-0c0c-45b2-afd8-755d83580019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shapes(orig_attns, edit_attns, indices, tau=1):\n",
    "    shapes = []\n",
    "    for o in indices:\n",
    "        deltas = []\n",
    "        for i in range(len(edit_attns)):\n",
    "            orig, edit = orig_attns[i][:,:,o], edit_attns[i][:,:,o]\n",
    "            delta = tau*get_shape(orig) - get_shape(edit)\n",
    "            deltas.append(delta.mean())\n",
    "        shapes.append(torch.stack(deltas).mean())\n",
    "    return torch.stack(shapes).mean()\n",
    "\n",
    "def fix_appearances(orig_attns, orig_feats, edit_attns, edit_feats, indices, attn_idx=-1):\n",
    "    appearances = []\n",
    "    for o in indices:\n",
    "        orig = torch.stack([a[:,:,o] for a in orig_attns[-3:]]).mean(0)\n",
    "        edit = torch.stack([a[:,:,o] for a in edit_attns[-3:]]).mean(0)\n",
    "        appearances.append((get_appearance(orig, orig_feats) - get_appearance(edit, edit_feats)).pow(2).mean())\n",
    "    return torch.stack(appearances).mean()\n",
    "\n",
    "def fix_sizes(orig_attns, edit_attns, indices, tau=1):\n",
    "    sizes = []\n",
    "    for i in range(len(edit_attns)):\n",
    "        orig, edit = orig_attns[i][:,:,indices], edit_attns[i][:,:,indices]\n",
    "        sizes.append(tau*get_size(orig) - get_size(edit))\n",
    "    return torch.stack(sizes).mean()\n",
    "\n",
    "def position_deltas(orig_attns, edit_attns, indices, target_centroid=None):\n",
    "    positions = []\n",
    "    for i in range(len(edit_attns)):\n",
    "        orig, edit = orig_attns[i][:,:,indices], edit_attns[i][:,:,indices]\n",
    "        target = tensor(target_centroid) if target_centroid is not None else get_centroid(orig)\n",
    "        positions.append(target.to(orig.device) - get_centroid(edit))\n",
    "    return torch.stack(positions).mean()\n",
    "\n",
    "def fix_selfs(origs, edits):\n",
    "    shapes = []\n",
    "    for i in range(len(edits)):\n",
    "        shapes.append((threshold_attention(origs[i]) - threshold_attention(edits[i])).mean())\n",
    "    return torch.stack(shapes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfb404-8e28-4b67-948d-603a83779f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attns(attn_storage, attn_type='attn2'):\n",
    "    origs = [v['orig'] for k,v in attn_storage.storage.items() if attn_type in k]\n",
    "    edits = [v['edit'] for k,v in attn_storage.storage.items() if attn_type in k]\n",
    "    return origs, edits\n",
    "\n",
    "def edit_layout(attn_storage, indices, appearance_weight=0.5, orig_feats=None, edit_feats=None, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    return appearance_weight*fix_appearances(origs, orig_feats, edits, edit_feats, indices, **kwargs)\n",
    "\n",
    "def edit_appearance(attn_storage, indices, shape_weight=1, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    return shape_weight*fix_shapes(origs, edits, indices)\n",
    "\n",
    "def resize_object(attn_storage, indices, relative_size=2, shape_weight=1, size_weight=1, appearance_weight=0.1, orig_feats=None, edit_feats=None, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    if len(indices) > 1: \n",
    "        obj_idx, other_idx = indices\n",
    "        indices = torch.cat([obj_idx, other_idx])\n",
    "    shape_term = shape_weight*fix_shapes(origs, edits, indices)\n",
    "    appearance_term = appearance_weight*fix_appearances(origs, orig_feats, edits, edit_feats, indices)\n",
    "    size_term = size_weight*fix_sizes(origs, edits, indices, tau=relative_size)\n",
    "    return shape_term + appearance_term + size_term\n",
    "\n",
    "def move_object(attn_storage, indices, target_centroid=None, shape_weight=1, size_weight=1, appearance_weight=0.5, position_weight=1, orig_feats=None, edit_feats=None, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    if len(indices) > 1: \n",
    "        obj_idx, other_idx = indices\n",
    "        indices = torch.cat([obj_idx, other_idx])\n",
    "    shape_term = shape_weight*fix_shapes(origs, edits, indices)\n",
    "    appearance_term = appearance_weight*fix_appearances(origs, orig_feats, edits, edit_feats, indices)\n",
    "    size_term = size_weight*fix_sizes(origs, edits, obj_idx)\n",
    "    position_term = position_weight*position_deltas(origs, edits, obj_idx, target_centroid=target_centroid)\n",
    "    return shape_term + appearance_term + size_term + position_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111350f-7d41-4a4e-8000-735423d3dbcd",
   "metadata": {},
   "source": [
    "#### Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b699d76-42f4-46c3-a464-0499c206b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_self_guidance(t, n, scheduler):\n",
    "    if type(scheduler).__name__ == \"DDPMScheduler\":\n",
    "        if t <= int((3*n)/16): return True\n",
    "        elif t >= int(n - n/32): return False\n",
    "        elif t % 2 == 0: return True\n",
    "        else: return False\n",
    "    elif type(scheduler).__name__ == \"LMSDiscreteScheduler\":\n",
    "        # return True\n",
    "        if t <= int(n/5): return True\n",
    "        elif t >= n - 5: return False\n",
    "        elif t % 2 == 0: return True\n",
    "        else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5876e6f-b226-4a98-9646-567163fe36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_word_indexes(prompt, tokeniser, object_to_edit=None, **kwargs):\n",
    "    \"\"\"Extracts token indexes by treating all words in the prompt as separate objects.\"\"\"\n",
    "    prompt_inputs = tokeniser(prompt, padding=\"max_length\", max_length=tokeniser.model_max_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    if object_to_edit is not None: \n",
    "        obj_inputs = tokeniser(object_to_edit, add_special_tokens=False).input_ids\n",
    "        obj_idx = torch.cat([torch.where(prompt_inputs == o)[1] for o in obj_inputs])\n",
    "        a = set(torch.cat([torch.where(prompt_inputs != o)[1] for o in obj_inputs]).numpy())\n",
    "        b = set(torch.where(prompt_inputs < 49405)[1].numpy())\n",
    "        other_idx = tensor(list(a&b))\n",
    "        return obj_idx, other_idx\n",
    "    else: return torch.where(prompt_inputs < 49405)[1]\n",
    "\n",
    "def choose_object_indexes(prompt, tokeniser, objects:list=None, object_to_edit=None):\n",
    "    \"\"\"Extracts token indexes only for user-defined objects.\"\"\"\n",
    "    prompt_inputs = tokeniser(prompt, padding=\"max_length\", max_length=tokeniser.model_max_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    if object_to_edit is not None: \n",
    "        obj_inputs = tokeniser(object_to_edit, add_special_tokens=False).input_ids\n",
    "        obj_idx = torch.cat([torch.where(prompt_inputs == o)[1] for o in obj_inputs])\n",
    "        if object_to_edit in objects: objects.remove(object_to_edit)\n",
    "    other_idx = []\n",
    "    for o in objects:\n",
    "        inps = tokeniser(o, add_special_tokens=False).input_ids\n",
    "        other_idx.append(torch.cat([torch.where(prompt_inputs == o)[1] for o in inps]))\n",
    "    if object_to_edit is None: return torch.cat(other_idx)\n",
    "    else: return obj_idx, torch.cat(other_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc3a7e-ea66-4672-82b9-5b7e0684de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_sample(\n",
    "    prompt,\n",
    "    model,\n",
    "    scheduler,\n",
    "    guidance_func,\n",
    "    g_weight=10, \n",
    "    feature_layer=None, \n",
    "    idx_func=all_word_indexes,\n",
    "    objects:list=None,\n",
    "    obj_to_edit=None,\n",
    "    use_same_seed=False,\n",
    "    seed=None, steps=50, guidance_scale=5., device='cuda', height=512, width=512, return_original=True\n",
    "):\n",
    "    if seed is None: seed = int(torch.rand((1,)) * 1000000)\n",
    "    seed_2 = int(torch.rand((1,)) * 1000000) if not use_same_seed else seed\n",
    "    \n",
    "    # set up the custom attn processor and use to replace standard model processors\n",
    "    storage = AttnStorage()\n",
    "    processor = partial(CustomAttnProcessor, storage)\n",
    "    attn_dict = get_attn_dict(processor, model)\n",
    "    model.set_attn_processor(attn_dict)\n",
    "    \n",
    "    # set up the hook to collect activations from feature_layer\n",
    "    g_name = guidance_func.func.__name__ if isinstance(guidance_func, partial) else guidance_func.__name__\n",
    "    if g_name not in ['edit_appearance'] and feature_layer is None:\n",
    "        feature_layer = model.up_blocks[-1].resnets[-2]\n",
    "    if feature_layer is not None: hook = Hook(feature_layer, get_features)\n",
    "    \n",
    "    # get indexes of editable and non-editable objects from token sequence\n",
    "    if idx_func.__name__ == 'choose_object_indexes' and objects is None:\n",
    "        raise ValueError('Provide a list of object strings from the prompt.')\n",
    "    if g_name not in ['edit_layout', 'edit_appearance', 'edit_layout_2'] and obj_to_edit is None:\n",
    "        raise ValueError('Provide an object string for editing.')\n",
    "    indices = idx_func(prompt, tokeniser, objects=objects, object_to_edit=obj_to_edit)\n",
    "    \n",
    "    # set up embeddings, latents and scheduler\n",
    "    uncond_embeddings = get_embeddings(\"\", concat_unconditional=False, device=device)\n",
    "    cond_embeddings = get_embeddings(prompt, concat_unconditional=False, device=device)\n",
    "    scheduler.set_timesteps(steps)\n",
    "    scheduler_2 = copy.deepcopy(scheduler)\n",
    "    shape = (1, model.config.in_channels, height // 8, width // 8)\n",
    "    orig_latents = torch.randn(shape, generator=torch.manual_seed(seed)).to(device) * scheduler.init_noise_sigma\n",
    "    edit_latents = torch.randn(shape, generator=torch.manual_seed(seed_2)).to(device) * scheduler.init_noise_sigma\n",
    "    \n",
    "    for i, t in enumerate(progress_bar(scheduler.timesteps, leave=False)):\n",
    "        # calculate noise_pred on the original unedited solution path\n",
    "        latent_model_input = scheduler.scale_model_input(orig_latents, t) ## note orig_latents\n",
    "        with torch.no_grad(): \n",
    "            # don't store attention for the uncond prediction\n",
    "            prepare_attention(model, storage, set_store=False)\n",
    "            uncond = model(latent_model_input, t, encoder_hidden_states=uncond_embeddings).sample\n",
    "\n",
    "            # do store attention for the cond prediction\n",
    "            prepare_attention(model, storage, pred_type='orig', set_store=True)\n",
    "            cond = model(latent_model_input, t, encoder_hidden_states=cond_embeddings).sample\n",
    "            orig_feats = hook.feats if feature_layer is not None else None\n",
    "        \n",
    "        # classifier-free guidance on original solution path\n",
    "        orig_noise_pred = uncond + guidance_scale * (cond - uncond)\n",
    "        orig_latents = scheduler.step(orig_noise_pred, t, orig_latents).prev_sample\n",
    "        \n",
    "        edit_latents.requires_grad_(True)\n",
    "        edit_latents.retain_grad()\n",
    "        \n",
    "        # recalculate noise_pred for edited solution path and allow grads to flow this time\n",
    "        latent_model_input = scheduler_2.scale_model_input(edit_latents, t) ## note edit_latents\n",
    "        prepare_attention(model, storage, set_store=False)\n",
    "        uncond = model(latent_model_input, t, encoder_hidden_states=uncond_embeddings).sample\n",
    "\n",
    "        prepare_attention(model, storage, pred_type='edit', set_store=True)\n",
    "        cond = model(latent_model_input, t, encoder_hidden_states=cond_embeddings).sample\n",
    "        edit_feats = hook.feats if feature_layer is not None else None\n",
    "        \n",
    "        # perform guidance with flexible g function\n",
    "        edit_noise_pred = uncond + guidance_scale * (cond - uncond)\n",
    "        if do_self_guidance(i, len(scheduler.timesteps), scheduler):\n",
    "            g = guidance_func(storage, indices, orig_feats=orig_feats, edit_feats=edit_feats)\n",
    "            g.backward()\n",
    "            sig_t = scheduler.sigmas[i]\n",
    "            edit_noise_pred += g_weight*sig_t*edit_latents.grad\n",
    "        edit_latents = scheduler_2.step(edit_noise_pred.detach(), t, edit_latents.detach()).prev_sample\n",
    "        storage.flush()\n",
    "        \n",
    "    orig_latents = 1 / 0.18215 * orig_latents\n",
    "    edit_latents = 1 / 0.18215 * edit_latents\n",
    "\n",
    "    with torch.no_grad(): edit_img = vae.decode(edit_latents).sample\n",
    "    if not return_original: return edit_img\n",
    "    with torch.no_grad(): orig_img = vae.decode(orig_latents).sample\n",
    "    return orig_img, edit_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8e281-b9d0-4905-8f7e-8bdbdd05317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_original(prompt, seed=None, height=512, width=512, steps=50, guidance_scale=5, device='cuda'):\n",
    "    if seed is None: seed = int(torch.rand((1,)) * 1000000)\n",
    "    embeddings = get_embeddings(prompt, concat_unconditional=True, device=device)\n",
    "    scheduler.set_timesteps(steps)\n",
    "    shape = (1, model.in_channels, height // 8, width // 8)\n",
    "    latents = torch.randn(shape, generator=torch.manual_seed(seed)).to(device)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    for i, t in enumerate(progress_bar(scheduler.timesteps, leave=False)):\n",
    "        latent_model_input = torch.cat([latents] * 2).to(device)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad(): image = vae.decode(latents).sample\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5048dbe-b277-4547-9a9e-ea65edc423f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='unet').to('cuda')\n",
    "tokeniser = AutoTokenizer.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='vae').to('cuda')\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3a0c6-c7ae-4ef7-9047-82e703b13a53",
   "metadata": {},
   "source": [
    "#### Sample new appearances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a711d-290b-4cab-a849-d681d05038f2",
   "metadata": {},
   "source": [
    "#### Sample new layouts\n",
    "\n",
    "Additional experimental code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0868f-490f-456f-9cb4-949595d40540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_appearances_2(orig_attns, orig_feats, edit_attns, edit_feats, indices, attn_idx=-1):\n",
    "    appearances = []\n",
    "    for o in indices: appearances.append((orig_feats - edit_feats).pow(2).mean())\n",
    "    return torch.stack(appearances).mean()\n",
    "\n",
    "def edit_layout_2(attn_storage, indices, appearance_weight=0.5, orig_feats=None, edit_feats=None, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    \n",
    "    return appearance_weight*fix_appearances_2(origs, orig_feats, edits, edit_feats, indices, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c569e-ae6a-459b-8938-5ab6543d4b2f",
   "metadata": {},
   "source": [
    "#### Move an object\n",
    "\n",
    "Additional experimental code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0b5b8-3334-4eb2-867d-2d9975ef7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_shape(x, direction='up', factor=0.5):\n",
    "    h = w = int(math.sqrt(x.shape[-2]))\n",
    "    mag = (0,0)\n",
    "    if direction == 'up': mag = (int(-h*factor),0)\n",
    "    elif direction == 'down': mag = (int(-h*factor),0)\n",
    "    elif direction == 'right': mag = (0,int(w*factor))\n",
    "    elif direction == 'left': mag = (0,int(-w*factor))\n",
    "    shape = (x.shape[0], h, h, x.shape[-1])\n",
    "    x = x.view(shape)\n",
    "    move = x.roll(mag, dims=(1,2))\n",
    "    return move.view(x.shape[0], h*h, x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5e6d3-bf78-497d-8654-d25054c7ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_shape(x, direction='up'): \n",
    "    h = w = int(math.sqrt(x.shape[-2]))\n",
    "    shape = (x.shape[0], h, w, x.shape[-1])\n",
    "    x = x.view(shape)\n",
    "    shift = torch.zeros_like(x)\n",
    "    \n",
    "    if direction == 'up':\n",
    "        shift[:, :h//4*3, :, :] = x[:, h//4:, :, :]\n",
    "        shift[:, h//4*3:, :, :] = x[:, :h//4, :, :]\n",
    "    elif direction == 'down':\n",
    "        shift[:, h//4:, :, :] = x[:, :h//4*3, :, :]\n",
    "        shift[:, :h//4, :, :] = x[:, h//4*3:, :, :]\n",
    "    elif direction == 'right':\n",
    "        shift[:, :, :w//4*3, :] = x[:, :, w//4:, :]\n",
    "        shift[:, :, w//4*3:, :] = x[:, :, :w//4, :]\n",
    "    elif direction == 'left':\n",
    "        shift[:, :, w//4:, :] = x[:, :, :w//4*3, :]\n",
    "        shift[:, :, :w//4, :] = x[:, :, w//4*3:, :]\n",
    "    \n",
    "    return shift.view(x.shape[0], h*h, x.shape[-1])\n",
    "\n",
    "# def shift_shape(x, direction='up'): \n",
    "#     h = w = int(math.sqrt(x.shape[-2]))\n",
    "#     shape = (x.shape[0], h, w, x.shape[-1])\n",
    "#     x = x.view(shape)\n",
    "#     shift = torch.zeros_like(x)\n",
    "    \n",
    "#     if direction == 'up':\n",
    "#         shift[:, :h//4, :, :] = x[:, h//4:, :, :]\n",
    "#     elif direction == 'down':\n",
    "#         shift[:, h//4:, :, :] = x[:, :h//4, :, :]\n",
    "#     elif direction == 'right':\n",
    "#         shift[:, :, :w//4, :] = x[:, :, w//4:, :]\n",
    "#     elif direction == 'left':\n",
    "#         shift[:, :, w//4:, :] = x[:, :, :w//4, :]\n",
    "    \n",
    "#     return shift.view(x.shape[0], h*h, x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddbf4a-3f97-4327-bf7d-62257dd16c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shapes_3(orig_attns, edit_attns, indices, tau=fc.noop):\n",
    "    shapes = []\n",
    "    for o in indices:\n",
    "        deltas = []\n",
    "        for i in range(len(edit_attns)):\n",
    "            orig, edit = orig_attns[i][:,:,o], edit_attns[i][:,:,o]\n",
    "            if len(orig.shape) < 3: orig, edit = orig[...,None], edit[...,None]\n",
    "            delta = (tau(get_shape(orig)) - get_shape(edit)).pow(2).mean()\n",
    "            deltas.append(delta.mean())\n",
    "        shapes.append(torch.stack(deltas).mean())\n",
    "    return torch.stack(shapes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4928d31-50d7-44c7-9e4d-c8822f7baac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_selfs_2(origs, edits, t=fc.noop):\n",
    "    deltas = []\n",
    "    for i in range(len(edits)):\n",
    "        orig, edit = origs[i][...,None].mean(0), edits[i]\n",
    "        delta = t(orig).squeeze() - edit\n",
    "        deltas.append(delta.mean())\n",
    "    return torch.stack(deltas).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0dc805-97a7-4c26-b1d9-79665b05e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_object(attn_storage, indices, t=fc.noop, shape_weight=1, size_weight=1, self_weight=0.1, appearance_weight=0.5, position_weight=1, orig_feats=None, edit_feats=None, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    # orig_selfs = [v['orig'] for k,v in attn_storage.storage.items() if 'attn1' in k and v['orig'].shape[-1] == 4096]\n",
    "    # edit_selfs = [v['edit'] for k,v in attn_storage.storage.items() if 'attn1' in k and v['orig'].shape[-1] == 4096]\n",
    "    if len(indices) > 1: \n",
    "        obj_idx, other_idx = indices\n",
    "        indices = torch.cat([obj_idx, other_idx])\n",
    "    shape_term = shape_weight*fix_shapes(origs, edits, obj_idx)\n",
    "    appearance_term = appearance_weight*fix_appearances_2(origs, orig_feats, edits, edit_feats, indices)\n",
    "    # size_term = size_weight*fix_sizes(origs, edits, obj_idx)\n",
    "    # position_term = position_weight*position_deltas_2(origs, edits, obj_idx, target_centroid=target_centroid)\n",
    "    # self_term = self_weight*fix_selfs_2(orig_selfs, edit_selfs, t=t)\n",
    "    move_term = position_weight*fix_shapes_3(origs, edits, other_idx, tau=t)\n",
    "    return move_term + shape_term + appearance_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be3bba-a811-4073-b116-3db5d3171600",
   "metadata": {},
   "source": [
    "#### Resize an object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d3f88-eba7-4061-b571-1f226e32f402",
   "metadata": {},
   "source": [
    "Additional experimental code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85052ad1-e5cd-4600-8cbf-bff93671fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge(x, scale_factor=1):\n",
    "    assert scale_factor >= 1\n",
    "    h = w = int(math.sqrt(x.shape[-2]))\n",
    "    x = rearrange(x, 'n (h w) d -> n d h w', h=h)\n",
    "    x = F.interpolate(x, scale_factor=scale_factor)\n",
    "    new_h = new_w = x.shape[-1]\n",
    "    x_l, x_r = (new_w//2) - w//2, (new_w//2) + w//2\n",
    "    x_t, x_b = (new_h//2) - h//2, (new_h//2) + h//2\n",
    "    x = x[:,:,x_t:x_b,x_l:x_r]\n",
    "    return rearrange(x, 'n d h w -> n (h w) d', h=h) * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee85bd5-5028-40a8-814a-effed8e48f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(x, scale_factor=1):\n",
    "    assert scale_factor <= 1\n",
    "    h = w = int(math.sqrt(x.shape[-2]))\n",
    "    x = rearrange(x, 'n (h w) d -> n d h w', h=h)\n",
    "    sf = int(1/scale_factor)\n",
    "    new_h, new_w = h*sf, w*sf\n",
    "    x1 = torch.zeros(x.shape[0], x.shape[1], new_h, new_w).to(x.device)\n",
    "    x_l, x_r = (new_w//2) - w//2, (new_w//2) + w//2\n",
    "    x_t, x_b = (new_h//2) - h//2, (new_h//2) + h//2\n",
    "    x1[:,:,x_t:x_b,x_l:x_r] = x\n",
    "    shrink = F.interpolate(x1, scale_factor=scale_factor)\n",
    "    return rearrange(shrink, 'n d h w -> n (h w) d', h=h) * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43561d8-0f45-4000-85d3-10fb27222d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x, scale_factor=1):\n",
    "    if scale_factor > 1: return enlarge(x)\n",
    "    elif scale_factor < 1: return shrink(x)\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53fe9f-9a82-4e4c-8d72-0978a4df4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shapes_2(orig_attns, edit_attns, indices, tau=1):\n",
    "    shapes = []\n",
    "    for o in indices:\n",
    "        deltas = []\n",
    "        for i in range(len(edit_attns)):\n",
    "            orig, edit = orig_attns[i][:,:,o], edit_attns[i][:,:,o]\n",
    "            t = orig + (orig.max() * tau)\n",
    "            delta = (get_shape((orig + t).clip(min=0))) - get_shape(edit)\n",
    "            deltas.append(delta.mean())\n",
    "        shapes.append(torch.stack(deltas).mean())\n",
    "    return torch.stack(shapes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5de8e-d4fa-4a72-9979-6301a11045d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_shapes_3(orig_attns, edit_attns, indices, tau=fc.noop):\n",
    "#     shapes = []\n",
    "#     for o in indices:\n",
    "#         deltas = []\n",
    "#         for i in range(len(edit_attns)):\n",
    "#             orig, edit = orig_attns[i][:,:,o], edit_attns[i][:,:,o]\n",
    "#             if len(orig.shape) < 3: orig, edit = orig[...,None], edit[...,None]\n",
    "#             delta = (tau(get_shape(orig)) - get_shape(edit)).pow(2).mean()\n",
    "#             deltas.append(delta.mean())\n",
    "#         shapes.append(torch.stack(deltas).mean())\n",
    "#     return torch.stack(shapes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d910a0-69cf-49f9-b536-7a8d5ef73e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_object_2(attn_storage, indices, t=fc.noop, relative_size=2, shape_weight=1, size_weight=1, appearance_weight=0.1, orig_feats=None, edit_feats=None, self_weight=0.1, **kwargs):\n",
    "    origs, edits = get_attns(attn_storage)\n",
    "    # orig_selfs = [v['orig'] for k,v in attn_storage.storage.items() if 'attn1' in k][-1]\n",
    "    # edit_selfs = [v['edit'] for k,v in attn_storage.storage.items() if 'attn1' in k][-1]\n",
    "    if len(indices) > 1:\n",
    "        obj_idx, other_idx = indices\n",
    "        indices = torch.cat([obj_idx, other_idx])\n",
    "    shape_term = shape_weight*fix_shapes(origs, edits, other_idx)\n",
    "    appearance_term = appearance_weight*fix_appearances(origs, orig_feats, edits, edit_feats, indices)\n",
    "    size_term = size_weight*fix_shapes_2(origs, edits, obj_idx, tau=t)\n",
    "    # self_term = self_weight*fix_selfs(orig_selfs, edit_selfs)\n",
    "    return shape_term + appearance_term + size_term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
